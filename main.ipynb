{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\grint\\anaconda3\\envs\\dc\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "import torchvision.models as models # 이미지\n",
    "\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer # 텍스트\n",
    "from transformers import BertModel, BertTokenizer # 텍스트\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cuda\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() : device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available() : device = torch.device('mps')\n",
    "else : device=torch.device('cpu')\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, transform, img_path, is_test=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.img_path = img_path\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img_name = os.path.join(self.img_path, row['image_id'] + '.jpg')\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "\n",
    "        image = self.transform(image)\n",
    "\n",
    "        question = row['question']\n",
    "        question = self.tokenizer(question, truncation=True, padding='max_length', max_length=70, return_tensors=\"pt\")\n",
    "\n",
    "        if not self.is_test:\n",
    "            answer = row['answer']\n",
    "\n",
    "            answer = self.tokenizer(answer, truncation=True, padding='max_length', max_length=70, return_tensors=\"pt\")\n",
    "            \n",
    "            return {\n",
    "                'image': image.squeeze(),\n",
    "                'question': question['input_ids'].squeeze(),\n",
    "                'answer': answer['input_ids'].squeeze()\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'image': image.squeeze(),\n",
    "                'question': question['input_ids'].squeeze(),\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "train_df = pd.read_csv('trainsformed_train.csv')\n",
    "test_df = pd.read_csv('trainsformed_test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "train_img_path = 'image/train'\n",
    "test_img_path = 'image/test'\n",
    "\n",
    "# dataset & dataloader\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((128, 128)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = VQADataset(train_df, tokenizer, transform, train_img_path, is_test=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=1, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(VQAModel, self).__init__()\n",
    "        if torch.cuda.is_available() : self.device = torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available() : self.device = torch.device('mps')\n",
    "        else : self.device=torch.device('cpu')\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "        # self.vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.vgg = models.resnet101()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.bert.pooler = nn.Linear(768, 768)\n",
    "\n",
    "        # combined_features_size = self.vit.classifier.out_features + self.flan.lm_head.out_features\n",
    "        combined_features_size = 1000 + self.bert.pooler.out_features\n",
    "\n",
    "        self.classifier = nn.Linear(combined_features_size, vocab_size)\n",
    "\n",
    "    def forward(self, images, question):\n",
    "        # inputs = self.vit_processor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "        # inputs = inputs.to(self.device)\n",
    "        # image_features = self.vit(**inputs).logits\n",
    "        image_features = self.vgg(images)\n",
    "\n",
    "        image_features = image_features.view(image_features.size(0),-1)\n",
    "\n",
    "        # question_features = self.flan(question,question_attention_mask,answer).logits\n",
    "        question_features = self.bert(question).last_hidden_state\n",
    "\n",
    "        image_features = image_features.unsqueeze(1).expand(-1, question_features.size(1),-1) # [batch, sequence, 1000]\n",
    "\n",
    "        combined = torch.cat([image_features, question_features], dim=-1) # [batch, sequence, 1000+hidden]\n",
    "\n",
    "        output = self.classifier(combined) # [batch, vocab_size]\n",
    "\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, accumulation_steps=8):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in tqdm(enumerate(loader), total=len(loader)):\n",
    "        images = data['image'].to(device)\n",
    "        question = data['question'].to(device)\n",
    "        answer = data['answer'].to(device)\n",
    "\n",
    "        optimizer.zero_grad() if i % accumulation_steps == 0 else None\n",
    "\n",
    "        outputs = model(images=images, question=question)\n",
    "\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), answer.view(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss /= accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = VQAModel(len(tokenizer)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 72/359521 [00:11<15:32:38,  6.42it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 12\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m     11\u001b[0m     epoch \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 12\u001b[0m     avg_loss \u001b[39m=\u001b[39m train(model, train_loader, optimizer, criterion)\n\u001b[0;32m     13\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEpoch: \u001b[39m\u001b[39m{\u001b[39;00mepoch\u001b[39m}\u001b[39;00m\u001b[39m, Loss: \u001b[39m\u001b[39m{\u001b[39;00mavg_loss\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n\u001b[0;32m     15\u001b[0m     \u001b[39m# 성능이 좋아질 때마다 모델 저장\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[6], line 6\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, loader, optimizer, criterion, accumulation_steps)\u001b[0m\n\u001b[0;32m      3\u001b[0m total_loss \u001b[39m=\u001b[39m \u001b[39m0\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[39mfor\u001b[39;00m i, data \u001b[39min\u001b[39;00m tqdm(\u001b[39menumerate\u001b[39m(loader), total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(loader)):\n\u001b[1;32m----> 6\u001b[0m     images \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39;49m\u001b[39mimage\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mto(device)\n\u001b[0;32m      7\u001b[0m     question \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39mquestion\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      8\u001b[0m     answer \u001b[39m=\u001b[39m data[\u001b[39m'\u001b[39m\u001b[39manswer\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mto(device)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\",exist_ok=True)\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "best_loss = float('inf')  # 초기 최적의 손실값을 무한대로 설정\n",
    "\n",
    "epoch = 0\n",
    "while True:\n",
    "    epoch += 1\n",
    "    avg_loss = train(model, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch: {epoch}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 성능이 좋아질 때마다 모델 저장\n",
    "    if avg_loss < best_loss:\n",
    "        torch.save(model.state_dict(), f\"models/{epoch}_{avg_loss}.pth\")\n",
    "        best_loss = avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
