{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "# from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "import torchvision.models as models # 이미지\n",
    "\n",
    "# from transformers import AutoModelForSeq2SeqLM, AutoTokenizer # 텍스트\n",
    "from transformers import BertModel, BertTokenizer # 텍스트\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "from PIL import Image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using mps\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available() : device = torch.device('cuda')\n",
    "elif torch.backends.mps.is_available() : device = torch.device('mps')\n",
    "else : device=torch.device('cpu')\n",
    "print(f'Using {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQADataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, transform, img_path, is_test=False):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.transform = transform\n",
    "        self.img_path = img_path\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        img_name = os.path.join(self.img_path, row['image_id'] + '.jpg')\n",
    "        image = Image.open(img_name).convert('RGB')\n",
    "\n",
    "        image = self.transform(image)\n",
    "\n",
    "        question = row['question']\n",
    "        question = self.tokenizer(question, truncation=True, padding='max_length', max_length=100, return_tensors=\"pt\")\n",
    "\n",
    "        if not self.is_test:\n",
    "            answer = row['answer']\n",
    "\n",
    "            answer = self.tokenizer(answer, truncation=True, padding='max_length', max_length=100, return_tensors=\"pt\")\n",
    "            \n",
    "            return {\n",
    "                'image': image.squeeze(),\n",
    "                'question': question['input_ids'].squeeze(),\n",
    "                'answer': answer['input_ids'].squeeze()\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'image': image.squeeze(),\n",
    "                'question': question['input_ids'].squeeze(),\n",
    "            }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 불러오기\n",
    "train_df = pd.read_csv('trainsformed_train.csv')\n",
    "test_df = pd.read_csv('trainsformed_test.csv')\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "train_img_path = 'image/train'\n",
    "test_img_path = 'image/test'\n",
    "\n",
    "# dataset & dataloader\n",
    "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "train_dataset = VQADataset(train_df, tokenizer, transform, train_img_path, is_test=False)\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VQAModel(nn.Module):\n",
    "    def __init__(self, vocab_size):\n",
    "        super(VQAModel, self).__init__()\n",
    "        if torch.cuda.is_available() : self.device = torch.device('cuda')\n",
    "        elif torch.backends.mps.is_available() : self.device = torch.device('mps')\n",
    "        else : self.device=torch.device('cpu')\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        # self.vit = ViTForImageClassification.from_pretrained('google/vit-base-patch16-224')\n",
    "        # self.vit_processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224')\n",
    "        self.vgg = models.resnet50(weights=True)\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "        self.bert.pooler = nn.Linear(768, 768)\n",
    "\n",
    "        # combined_features_size = self.vit.classifier.out_features + self.flan.lm_head.out_features\n",
    "        combined_features_size = 1000 + self.bert.pooler.out_features\n",
    "\n",
    "        self.classifier = nn.Linear(combined_features_size, vocab_size)\n",
    "\n",
    "    def forward(self, images, question):\n",
    "        # inputs = self.vit_processor(images=images, return_tensors=\"pt\")\n",
    "\n",
    "        # inputs = inputs.to(self.device)\n",
    "        # image_features = self.vit(**inputs).logits\n",
    "        image_features = self.vgg(images)\n",
    "\n",
    "        image_features = image_features.view(image_features.size(0),-1)\n",
    "\n",
    "        # question_features = self.flan(question,question_attention_mask,answer).logits\n",
    "        question_features = self.bert(question).last_hidden_state\n",
    "\n",
    "        image_features = image_features.unsqueeze(1).expand(-1, question_features.size(1),-1) # [batch, sequence, 1000]\n",
    "\n",
    "        combined = torch.cat([image_features, question_features], dim=-1) # [batch, sequence, 1000+hidden]\n",
    "\n",
    "        output = self.classifier(combined) # [batch, vocab_size]\n",
    "\n",
    "        return output\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, loader, optimizer, criterion, accumulation_steps=8):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, data in tqdm(enumerate(loader), total=len(loader)):\n",
    "        images = data['image'].to(device)\n",
    "        question = data['question'].to(device)\n",
    "        answer = data['answer'].to(device)\n",
    "\n",
    "        optimizer.zero_grad() if i % accumulation_steps == 0 else None\n",
    "\n",
    "        outputs = model(images=images, question=question)\n",
    "\n",
    "        loss = criterion(outputs.view(-1, outputs.size(-1)), answer.view(-1))\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        loss /= accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "\n",
    "    avg_loss = total_loss / len(loader)\n",
    "    return avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/hp/lib/python3.10/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = VQAModel(len(tokenizer)).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/44941 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "os.makedirs(\"models\",exist_ok=True)\n",
    "# Criterion and Optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\n",
    "# Training loop\n",
    "best_loss = float('inf')  # 초기 최적의 손실값을 무한대로 설정\n",
    "\n",
    "epoch = 0\n",
    "while True:\n",
    "    epoch += 1\n",
    "    avg_loss = train(model, train_loader, optimizer, criterion)\n",
    "    print(f\"Epoch: {epoch}, Loss: {avg_loss:.4f}\")\n",
    "\n",
    "    # 성능이 좋아질 때마다 모델 저장\n",
    "    if avg_loss < best_loss:\n",
    "        torch.save(model.state_dict(), f\"models/{epoch}_{avg_loss}.pth\")\n",
    "        best_loss = avg_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PATH = \"models/7_0.024020754328503088.pth\"\n",
    "model.load_state_dict(torch.load(PATH,map_location=device))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference(model, loader):\n",
    "    model.eval()\n",
    "    preds = []\n",
    "    with torch.no_grad():\n",
    "        for data in tqdm(loader, total=len(loader)):\n",
    "            images = data['image'].to(device)\n",
    "            question = data['question'].to(device)\n",
    "\n",
    "            outputs = model(images, question) # [batch, sequence, vocab]\n",
    "\n",
    "            _, pred = torch.max(outputs, dim=2) # values, indices = _, pred\n",
    "            preds.extend(pred.cpu().numpy())\n",
    "\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5452d6bd105b46dfad207380f4650f91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/20240 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Dataset & DataLoader\n",
    "test_dataset = VQADataset(test_df, tokenizer, transform, test_img_path, is_test=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=2, shuffle=False)\n",
    "\n",
    "# inference\n",
    "preds = inference(model, test_loader)\n",
    "\n",
    "# no_pad_output = []\n",
    "# for pred in preds:\n",
    "#     output = pred[pred != 50257] # [PAD] token 제외\n",
    "#     no_pad_output.append(tokenizer.decode(output).strip()) # 토큰 id -> 토큰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = []\n",
    "for pred in preds:\n",
    "    res.append(tokenizer.decode(pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
